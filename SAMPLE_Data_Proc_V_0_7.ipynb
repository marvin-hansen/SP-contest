{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock-Data-Proc-Fast.ai-V-0.7",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "B2fYAS698gxy",
        "G8YfeI5K_7d8",
        "wFPnQqXD8bqF",
        "a9ZyTvLnmNB9",
        "QMfsFdovmreG",
        "b2lIErS98ff6",
        "SksnSvn1M1OD",
        "EH8oJfFe7wP_",
        "j7ep0eFcGk0s",
        "fPM7CMOiFgq7",
        "r9T6v775Frr6",
        "Nj4D8EbVG8Hq",
        "IpuZT_EALwmL"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marvin-hansen/SP-contest/blob/master/SAMPLE_Data_Proc_V_0_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hyTJpkZTknaI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data pre-processing for S&P prediction with fast.ai\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "|    \t|               \t|\n",
        "|---------\t|-------------------------\t|\n",
        "| Author  \t| Marvin Hansen           \t|\n",
        "| Contact \t| marvin.hansen@gmail.com \t|\n",
        "| Version \t| 0.7.3                     \t|\n",
        "| Updated \t| March 29, 2019          \t|\n",
        "\n",
        "\n",
        "\n",
        "## Summary\n",
        "---\n",
        "\n",
        "Dataset and sample pre-processing pipeline for the S&P 500  prediction challange. This notebook contains a sample data processing pipeline consisting of the following tasks:\n",
        "\n",
        "1. Loading S&P500 (50Y) dataset \n",
        "2. Check for missing values (none in the 50Y set)\n",
        "3. Apply data transformation\n",
        "  * Rename coloumns \n",
        "  * Change data coloumn format to datetime \n",
        "  * Categorify date to capture trends and seasons\n",
        "  * Remove columns \n",
        "4. Feature engineering\n",
        "  * Calculate technical indicators (MACD, RSI, etc)\n",
        "  * Display feature matrix / heatmap \n",
        "5. Data processing\n",
        "  * Replacing NaN with zero (required for TabularLearner)\n",
        "  * Split data into train, test, validation sets\n",
        "  * Store all three datasets in a single zipfile \n",
        "\n",
        "\n",
        "## Getting started\n",
        "---\n",
        "1. Open notebook in colab \n",
        "2. Run install cells \n",
        "3. Run all cells (Ctrl-F9)\n",
        "4. Set Flag \"run\" to true and run the processing pipeline\n",
        "5. Have fun :-)\n",
        "\n",
        "\n",
        "## Install \n",
        "Before running the notebook:  \n",
        "1. Install all requirements by running all install cells\n",
        "2. Set install flags to false \n",
        "3. Restart envorironment otherwise imports fail\n",
        "\n",
        "\n",
        "\n",
        "## Known issues:\n",
        "---\n",
        "\n",
        "**Error:** \n",
        "TypeError: load_data() missing 1 required positional argument: 'path'\n",
        "\n",
        "**FIX:** \n",
        "Just re-run the  cell  with the  \"\" load_data()\" function\"  and run the pipeline again. \n",
        "\n",
        "\n",
        "\n",
        "## Changelog\n",
        "---\n",
        "* V0.7.3 added missing install flags & added install documentation. Added zip download. Fixed many small issues and glitches.  \n",
        "* V0.7 updated stock_data_processor\n",
        "* V0.6 updated utils\n",
        "* V-0.5  Added  initial data processing \n",
        "\n",
        "## Compatibility\n",
        "---\n",
        "\n",
        "Lib's\n",
        "* Python Version: 3.6.7\n",
        "* Pandas Version: 0.24.2\n",
        "* Numpy Version: 1.16.2\n",
        "* TA Lib Version: 0.4.17\n",
        "* FastAI Version: 1.0.50.post1\n",
        "* PyTorch Version: 1.0.1.post2\n",
        "\n",
        "GPU Acceleration\n",
        "* GPU: NVDIA K80 \n",
        "* Cuda V10.0.130\n"
      ]
    },
    {
      "metadata": {
        "id": "GOquBtIEJCrZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# The Data\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Ticker:**  S&P 500 \n",
        "\n",
        "**Time Frame:**  Daily closing price \n",
        "\n",
        "**Time range:** ~50 years \n",
        "\n",
        "***Start Date: ** Dec/31/1969\n",
        "\n",
        "***End Date: ** Mar/25/2019\n",
        "\n",
        "\n",
        "**Data Source:**: [finance.yahoo.com](https://finance.yahoo.com/quote/%5EGSPC/history?period1=-3600&period2=1553554800&interval=1d&filter=history&frequency=1d)\n",
        "\n",
        "** Data Storage:** [Github](https://github.com/marvin-hansen/SP-contest)\n",
        "\n",
        "**Source file:** SP500-50Y-raw.csv\n",
        "\n",
        "** Data Format:** CSV File \n",
        "\n",
        "\n",
        "\n",
        "**Train, Test, and Validation Datasets**\n",
        "\n",
        "* **Total size:** 12419\n",
        "* **Train/Test Split:** 80/20  \n",
        "* **Train:** 9935 entries\n",
        "* **Test:** 2484 entries\n",
        "* **Valid:** 90 entries\n",
        "\n",
        "\n",
        "##  Data Fields in source file \n",
        "\n",
        "|   Name\t| Type     \t|   \t Comment\n",
        "| :---\t| :---\t| :-:\t| :---\t\t\t|\n",
        "|  date \t| object    \t| Date of record \n",
        "|  Open | float64 | Open price |\n",
        "|  High | float64 | Highest day price |\n",
        "|  Low  | float64 | Lowest day price |\n",
        "|  Close  | float64 | Closing day  price |\n",
        "| Adj  Close  | float64 | Adjusted close  price |\n",
        "|  Volume  | int64 | Trade volume |\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3OvHntK3E0Fl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Flags "
      ]
    },
    {
      "metadata": {
        "id": "OYG0aoMoo3sk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Before running the notebook:  \n",
        "# 1) install all requirements by running the cells in hte install section \n",
        "# 2) set install flags to false \n",
        "# 3) Restart envorironment otherwise imports fail\n",
        "\n",
        "# Once done, run the entire notebook with Crl-F9\n",
        "\n",
        "# Flags for installation. Set false after install\n",
        "install_TA = True # ta-lib compiles forever... get a coffee\n",
        "install_RQ = True # install all other requirements \n",
        "install_QD = True # Quandl  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sndOy3Lm5RmB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Start & end data for QUANDL \n",
        "import datetime\n",
        "start = datetime.date(2008, 3, 29)  #year, month, day\n",
        "end = datetime.date.today()  \n",
        "#SET  API key\n",
        "# https://docs.quandl.com/docs/getting-started\n",
        "quandl.ApiConfig.api_key = \"YourKey\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zGv3bo9ERJtC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Run processor **"
      ]
    },
    {
      "metadata": {
        "id": "0epLuiL6RIdz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# flags for the processing pipline\n",
        "verbose = True\n",
        "persist = True # store resulting train, test & valid files to disk \n",
        "download = False # set to true to download the generated datasets \n",
        "\n",
        "\n",
        "run = False # set true  to run the entire  processing pipeline \n",
        "if run:\n",
        "    process_sp90y(verbose, rank, persist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DVuY19dt8JSn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ]
    },
    {
      "metadata": {
        "id": "B2fYAS698gxy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### numpy, pandas & fast.ai"
      ]
    },
    {
      "metadata": {
        "id": "2-1YmAhR9GBG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if(install_RQ): \n",
        "  \n",
        "  # set correct version\n",
        "  !pip install imgaug==0.2.7 \n",
        "\n",
        "  # update pandas \n",
        "  !pip install --upgrade pandas \n",
        "\n",
        "  # update numpy \n",
        "  !pip install --upgrade numpy "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Yy4octp9X4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if(install_RQ): \n",
        "  \n",
        "  # install latet fast.ai release\n",
        "  # https://github.com/fastai/fastai/releases\n",
        "  #!conda install -c pytorch -c fastai fastai\n",
        "  !curl -s https://course.fast.ai/setup/colab | bash\n",
        "  !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8YfeI5K_7d8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install TA/Lib for Technical Analysis"
      ]
    },
    {
      "metadata": {
        "id": "vne8ipPlACus",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if(install_TA):\n",
        "  \n",
        "\n",
        "  # https://colab.research.google.com/drive/1bRdCYAejMOgVZyCAr2F0kx6xw4v5SpIZ#scrollTo=zfTJzr8UtL96\n",
        "  # download TA-Lib \n",
        "  !wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "\n",
        "  !tar -xzvf ta-lib-0.4.0-src.tar.gz\n",
        "\n",
        "  # https://colab.research.google.com/drive/1bRdCYAejMOgVZyCAr2F0kx6xw4v5SpIZ#scrollTo=zfTJzr8UtL96\n",
        "  import os\n",
        "  os.chdir('ta-lib') # Can't use !cd in colab\n",
        "  !./configure --prefix=/usr\n",
        "  !make\n",
        "  !make install\n",
        "  # wait ~ 30s\n",
        "\n",
        "  os.chdir('../')\n",
        "  !pip install TA-Lib # finally...#\n",
        "  # cleanup \n",
        "  !rm -r ta-lib\n",
        "  !rm ta-*\n",
        "  !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rPdUUP3QOhmG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wFPnQqXD8bqF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install Quandl"
      ]
    },
    {
      "metadata": {
        "id": "gZO00mV6qU16",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if(install_TA):  \n",
        "  !pip install quandl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxhVyurpy3Xe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "a9ZyTvLnmNB9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ]
    },
    {
      "metadata": {
        "id": "YVfM_vuVl61s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "import platform\n",
        "import datetime\n",
        "import quandl\n",
        "import warnings\n",
        "from enum import Enum, unique\n",
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from google.colab import files # for file up & download. See end of notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import datetime, DataFrame\n",
        "from pandas.io.parsers import TextFileReader\n",
        "\n",
        "\n",
        "# fast ai \n",
        "import fastai\n",
        "from fastai import *\n",
        "from fastai.imports import *\n",
        "from fastai.basics import *\n",
        "from fastai.tabular import *\n",
        "from fastai.metrics import *\n",
        "\n",
        "import torch\n",
        "import talib\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3iWlhkqBXz8n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Check Version "
      ]
    },
    {
      "metadata": {
        "id": "Vf_31oMZl7gI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"* Python Version: \" + str(platform.python_version()))\n",
        "print(\"* Pandas Version: \" + str(pd.__version__))\n",
        "print(\"* Numpy Version: \" + str(np.__version__))\n",
        "\n",
        "print(\"* TA Lib Version: \" + str(talib.__version__))\n",
        "print(\"* FastAI Version: \" + str(fastai.__version__))\n",
        "print(\"* PyTorch Version: \" + str(torch.__version__))\n",
        "print()\n",
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RLvRy1G9mWdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Verify GPU *acceleration*"
      ]
    },
    {
      "metadata": {
        "id": "Z5S3z9YkmFqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.current_device()\n",
        "\n",
        "print(\"Cuda available: \" + str(torch.cuda.is_available()))\n",
        "print(\"Cuda enabled:\" + str(torch.backends.cudnn.enabled))\n",
        "\n",
        "#https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(\"GPU used: \" + torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMfsFdovmreG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ]
    },
    {
      "metadata": {
        "id": "ZIo-eDWrkYBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_nJKI6nkqEO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#batch size \n",
        "bs = 64\n",
        "# bs = 32 \n",
        "# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0P9oBhICOgHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.seed.html\n",
        "np.random.seed(42) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b2lIErS98ff6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ]
    },
    {
      "metadata": {
        "id": "SksnSvn1M1OD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Enums"
      ]
    },
    {
      "metadata": {
        "id": "uZFvFbiRFTli",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## file utils\n",
        "@unique\n",
        "class Data(Enum):\n",
        "    SP500_50Y_RAW = 0\n",
        "    SP500_90Y_RAW = 1\n",
        "    SP500_ALL = 2\n",
        "    SP500_TRAIN = 3\n",
        "    SP500_TEST = 4\n",
        "    SP500_VALID = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EH8oJfFe7wP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quandl Data Fetcher \n"
      ]
    },
    {
      "metadata": {
        "id": "DBAIa8NI8laC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "   # Treasury Yield Curve Rates\n",
        "    # https://www.quandl.com/data/USTREASURY/YIELD-Treasury-Yield-Curve-Rates\n",
        "    BOND_CODE = 'USTREASURY/'\n",
        "    BOND = ['YIELD']\n",
        "    \n",
        "     # S&P 500 investor Sentiment\n",
        "    # https://www.quandl.com/data/AAII/AAII_SENTIMENT-AAII-Investor-Sentiment-Data\n",
        "    # earliesr record: 1987/6/26\n",
        "    SP_SENT_Code = 'AAII/'\n",
        "    SP_SENT = 'AAII_SENTIMENT'\n",
        "    \n",
        "    # Yale  Sentiment data\n",
        "    SEN_YALE_CODE = \"YALE/\"\n",
        "    SEN_YALE = ['US_CONF_INDEX_VAL_INDIV', 'US_CONF_INDEX_VAL_INST', 'SPCOMP']\n",
        "    \n",
        "    # US Misery Index\n",
        "    SEN_MIS_CODE = \"USMISERY/\"\n",
        "    SEN_MIS = ['INDEX']\n",
        "    \n",
        "    # Housing data from NAHB\n",
        "    # https://www.quandl.com/data/NAHB-US-Housing-Indices\n",
        "    HOUSING_CODE = \"NAHB/\"\n",
        "    HOUSING = [\"HOUSEACT\", 'INTRATES', 'NWFHMI']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oMsMpRoU74gu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pull_SINGLE_STOCK(code, ticker, folder):\n",
        "\n",
        "    print(\"Pulling: \" + ticker)\n",
        "    pullstocks(code, ticker, folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnk9CAM99VI1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pullstocks(code, stock, folder):\n",
        "  \n",
        "\n",
        "    # https://medium.com/python-data/quandl-getting-end-of-day-stock-data-with-python-8652671d6661\n",
        "    for i in range(len(stock)):\n",
        "        data = quandl.get_table(code, ticker=stock[i],\n",
        "                                date={'gte': start, 'lte': end},\n",
        "                                paginate=True)\n",
        "\n",
        "        data.to_csv(folder + stock[i] + \"-Historical-Data.csv\")\n",
        "\n",
        "        print(stock[i] + \" -- DONE\")\n",
        "\n",
        "    print(\"Done!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85gYjevi9lEQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pull_Bond():\n",
        "    # Treasury Yield Curve Rates\n",
        "    label = \"Treasury Yield Curve Rates\"\n",
        "    folder = \"\"\n",
        "    print(\"Pulling: \" + label)\n",
        "    # \n",
        "    pullstocks(BOND_CODE, BOND, folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dCcEZDx49_k1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pull_SEN():\n",
        "    # Get AAII Investor Sentiment\n",
        "    label = \"Sentiment\"\n",
        "    folder = \"\"\n",
        "    print(\"Pulling: \" + label)\n",
        "    \n",
        "    # AAI\n",
        "    stocks = SP_SENT\n",
        "    code = SP_SENT_Code\n",
        "    pullstocks(code, stocks, folder)\n",
        "\n",
        "    # Yale Sentiment\n",
        "    stocks = SEN_YALE\n",
        "    code = SEN_YALE_CODE\n",
        "    pullstocks(code, stocks, folder)\n",
        "\n",
        "    # US MISERY INDEX\n",
        "    stocks = SEN_MIS_CODE\n",
        "    code = SEN_MIS\n",
        "    pullstocks(code, stocks, folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j7ep0eFcGk0s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## View & Visualize "
      ]
    },
    {
      "metadata": {
        "id": "v6d0X2HLGnjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_correlation_matrix(df):\n",
        "    \"\"\"\n",
        "    Shows a correlation matrix and a heatmap of the features in the data frame \n",
        "    :param df: pandas data frame \n",
        "    :return: void \n",
        "    \"\"\"\n",
        "    # copied from\n",
        "    # https://datascience.stackexchange.com/questions/10459/calculation-and-visualization-of-correlation-matrix-with-pandas\n",
        "    from matplotlib import pyplot as plt\n",
        "    from matplotlib import cm as cm\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(111)\n",
        "    cmap = cm.get_cmap('jet', 30)\n",
        "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
        "    ax1.grid(True)\n",
        "    plt.title(\"Feature Correlation\")\n",
        "    labels = df.columns.values.tolist()\n",
        "    ax1.set_xticklabels(labels, fontsize=6)\n",
        "    ax1.set_yticklabels(labels, fontsize=6)\n",
        "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
        "    fig.colorbar(cax, ticks=[.50, .55, .60, .65, .70, .75, .80, .85, .90, .95, 1])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2lmeqW_pSD3o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_versions():\n",
        "    print(\"* Python Version: \" + str(platform.python_version()))\n",
        "    print(\"* Pandas Version: \" + str(pd.__version__))\n",
        "    print(\"* Numpy Version: \" + str(np.__version__))\n",
        "    print(\"* TA Lib Version: \" + str(talib.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fPM7CMOiFgq7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load & Save "
      ]
    },
    {
      "metadata": {
        "id": "ba_85Ek3HwYQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_path(data_name: Data, url: bool):\n",
        "    \"\"\"\n",
        "    Returns the path corresponding to the data set specified in the enum Data.\n",
        "    Note, the enum is @unique so no two datasets can have the same path.\n",
        "\n",
        "    ONLY \"raw\" data have web url's to download the official reference dataset.\n",
        "    train, test, validate, and all are generated files.\n",
        "\n",
        "    When URL = True, the corresponding web url for the data set will be returned.\n",
        "\n",
        "    By default, path is relative /Data/filename.end\n",
        "\n",
        "    Update data_folder to set a different path.\n",
        "\n",
        "    :param data_name: Enum - Dataset\n",
        "    :param url: bool flag to indicate whether to return a local path or a web url\n",
        "    :return: file path or url\n",
        "    \"\"\"\n",
        "\n",
        "    data_folder = \"\" #\"Data/\"\n",
        "    sp_name = \"SP500\"\n",
        "    sp50_name = \"SP500-50Y\"\n",
        "    sp90_name = \"SP500-90Y\"\n",
        "    frmt = \".csv\"\n",
        "\n",
        "    path = \"\"\n",
        "\n",
        "    if (data_name is Data.SP500_50Y_RAW):\n",
        "        path = data_folder + sp50_name + \"-raw\" + frmt\n",
        "        if (url):\n",
        "            u = \"https://raw.githubusercontent.com/marvin-hansen/SP-contest/master/Data/SP500-50Y-raw.csv\"\n",
        "            path = requests.get(u).content\n",
        "\n",
        "    if (data_name is Data.SP500_90Y_RAW):\n",
        "        path = data_folder + sp90_name + \"-raw\" + frmt\n",
        "        if (url):\n",
        "            u = \"https://raw.githubusercontent.com/marvin-hansen/SP-contest/master/Data/SP500-90Y-raw.csv\"\n",
        "            path = requests.get(u).content\n",
        "\n",
        "\n",
        "    if (data_name is Data.SP500_ALL):\n",
        "        path = data_folder + sp_name + \"-all\" + frmt\n",
        "        if (url):\n",
        "            u = \"\"\n",
        "            path = requests.get(u).content\n",
        "    if (data_name is Data.SP500_TRAIN):\n",
        "        path = data_folder + sp_name + \"-train\" + frmt\n",
        "        if (url): path = \"\"\n",
        "    if (data_name is Data.SP500_TEST):\n",
        "        path = data_folder + sp_name + \"-test\" + frmt\n",
        "        if (url): path = \"\"\n",
        "    if (data_name is Data.SP500_VALID):\n",
        "        path = data_folder + sp_name + \"-valid\" + frmt\n",
        "        if (url): path = \"\"\n",
        "\n",
        "    return path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zYYrYDKsFYCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_csv_file(data_name: Data, url: bool):\n",
        "    \"\"\" loads the S&P 500 index file from the path in the path function\n",
        "    :param path:\n",
        "    :return: pandas data frame\n",
        "    \"\"\"\n",
        "    if url:\n",
        "        return pd.read_csv(io.StringIO(get_path(data_name=data_name, url=url).decode('utf-8')),infer_datetime_format=True)\n",
        "\n",
        "    else:\n",
        "        return pd.read_csv(get_path(data_name=data_name, url=url), infer_datetime_format=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8xrV9U8JFj1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(data: Data, force_download: bool = False):\n",
        "    \"\"\" Loads the requested dataset, either from the web or from a local copy.\n",
        "    The data loader stores a local copy of each file it loads from an URL to accelerate\n",
        "    the next loading of the same file. The data loader overrides the local copy\n",
        "    whenever force download is set to true.  By default, force is set to False\n",
        "    as to use the local copy first.\n",
        "\n",
        "    @depends: Data - Enum that specifies available datasets\n",
        "\n",
        "    @depends: get_path Adjust local file path and URL's.\n",
        "    Default relative path is data/\n",
        "    Default  URL is public github repo.\n",
        "\n",
        "    :param data: dataset to load\n",
        "    :param force_download: Download the web-version and override local copy. FALSE by default.\n",
        "    :return: pandas dataframe\n",
        "    \"\"\"\n",
        "    #\n",
        "    path = Path(get_path(data_name=data, url=False))\n",
        "    if(force_download or path.exists()== False):\n",
        "        print(\"Load from URL\")\n",
        "        df = load_csv_file(data_name=data, url=True)\n",
        "        # ... store a local copy to accelerate the next data loading\n",
        "        #df.to_csv(get_path(data_name=data, url=False))\n",
        "        return df\n",
        "    else: # local copy must be there b/c path exists\n",
        "        # load\n",
        "        print(\"Load data from local  file\")\n",
        "        return load_csv_file(data_name=data, url=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFs4IQ2gezzk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_train_test_valid(df: DataFrame, split_ratio: float, valid_size: int, verbose: bool):\n",
        "    \"\"\"\n",
        "    splits a dataframe into train, test, and validation and stores each set in a different file\n",
        "    :param df: pandas data frame\n",
        "    :param split_ratio: ratio between train & split\n",
        "    :param valid_size: number of rows in the validation set\n",
        "    :param verbose: prints out file paths when set to true\n",
        "    :return: void\n",
        "    \"\"\"\n",
        "    if (verbose):\n",
        "        print(\"Save data to file.. \")\n",
        "    \n",
        "    # replace NaN with zero\n",
        "    df = fill_nan(df)\n",
        "    \n",
        "    # store validation set as the latest of n data points\n",
        "    valid = df.head(valid_size)\n",
        "    valid_file = get_path(data_name=Data.SP500_VALID, url=False)\n",
        "    valid.to_csv(valid_file)\n",
        "\n",
        "    # split remaining data into train & test sets\n",
        "    split = int(len(df) * split_ratio)\n",
        "    train = df[0:split]  #\n",
        "    test = df[split:len(df)]\n",
        "  \n",
        "    ## store train dataset\n",
        "    train_file = get_path(data_name=Data.SP500_TRAIN, url=False)\n",
        "    train.to_csv(train_file)\n",
        "\n",
        "    # store train\n",
        "    test_file = get_path(data_name=Data.SP500_TEST, url=False)\n",
        "    test.to_csv(test_file)\n",
        "\n",
        "    if (verbose):\n",
        "        print('All data : %d' % (len(df)))\n",
        "        print('Training data: %d' % (len(train)))\n",
        "        print('Testing data: %d' % (len(test)))\n",
        "        print('validation data: %d' % (len(valid)))\n",
        "        print()\n",
        "        print(\"Stored train data in file: \")\n",
        "        print(train_file)\n",
        "        print()\n",
        "        print(\"Stored train data in file: \")\n",
        "        print(test_file)\n",
        "        print()\n",
        "        print(\"Stored validation data in file: \")\n",
        "        print(valid_file)\n",
        "        print()\n",
        "        print(\"Done! All data are saved\")\n",
        "        print(valid_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r9T6v775Frr6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataframe utils"
      ]
    },
    {
      "metadata": {
        "id": "ZJO0HQLgfjd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean(verbose: bool):\n",
        "    \"\"\"\n",
        "    Checks for missing values and removes them\n",
        "    :param verbose: print out details\n",
        "    :return: void - operates in place\n",
        "    \"\"\"\n",
        "    # load from file\n",
        "    df = load_csv(get_path(Data.SP500_90Y_RAW))\n",
        "\n",
        "    # check raw data for missing values\n",
        "    missing = check_missing_values(df=df, verbose=verbose)\n",
        "    # The S&P dataset has 45 missing values, which is barely 0.18% thus removing them won't hurt much.\n",
        "    if (missing):\n",
        "        df = remove_missing_values(df)\n",
        "        # Double check, should print zero\n",
        "        check_missing_values(df=df, verbose=verbose)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "boj-9g83fp_d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def check_missing_values(df, verbose: bool):\n",
        "    \"\"\"\n",
        "    Checks the given data frame for missing value and returns a boolean value.\n",
        "    When set to verbose, the function prints out number and percentage of missing data\n",
        "\n",
        "    :param df: pandas dataframe\n",
        "    :param verbose: boolean\n",
        "    :return: True if df contains missing values, otherwise false.\n",
        "    \"\"\"\n",
        "\n",
        "    # to get the total summation of all missing values in the DataFrame,\n",
        "    # #we chain two .sum() methods together:\n",
        "    if (verbose):\n",
        "        nr_missing = df.isnull().sum().sum()\n",
        "        nr_values = len(df)\n",
        "        prct_missing = (nr_missing / nr_values) * 100\n",
        "\n",
        "        print(\"Has missing values: \" + str(df.isnull().values.any()))\n",
        "        # inspect each column\n",
        "        col_names = df.columns.values.tolist()\n",
        "        for c in col_names:\n",
        "            print(str(c) + \" missing values: \" + str(df[c].isnull().sum()))\n",
        "        print()\n",
        "        print(\"Total Values: \" + str(nr_values))\n",
        "        print(\"Total Missing: \" + str(nr_missing))\n",
        "        print(\"Percentage Missing: \" + str(prct_missing))\n",
        "        print()\n",
        "\n",
        "    return df.isnull().values.any()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHiu0s-PfmRM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_missing_values(df):\n",
        "    \"\"\"\n",
        "    Drop each row in a data frame where at least one element is missing\n",
        "    and returns a copy without missing values.\n",
        "    :param df: pandas data frame\n",
        "    :return: df without missing values.\n",
        "    \"\"\"\n",
        "    return df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qSHqdktBZQJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_column(df, col_name):\n",
        "    \"\"\"\n",
        "    Deletes the given column(s) on the given data frame\n",
        "    :param df: pandas data frame\n",
        "    :param col_name: string array of column names\n",
        "    :return: data frame without the columns\n",
        "    \"\"\"\n",
        "    df.drop(columns=col_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "58te0Bl5ffVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fill_nan(df):\n",
        "    \"\"\" Fills NaN values with zero\n",
        "    :param df: pandas dataframe\n",
        "    :return: dataframe  without NaN\n",
        "    \"\"\"\n",
        "    return df.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sSaHTa6yFzKx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reverse_df(df):\n",
        "    \"\"\"\n",
        "    reverses all rows so that the last one are listed first\n",
        "    :param df: pandas data frame \n",
        "    :return: reversed frame\n",
        "    \"\"\"\n",
        "    return df.iloc[::-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3dUDmprkGMxS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rename_column(df, old_name, new_name):\n",
        "    \"\"\"\n",
        "    renames a column in the given data frame \n",
        "    :param df: pandas dataframe \n",
        "    :param old_name: \n",
        "    :param new_name: \n",
        "    :return: void \n",
        "    \"\"\"\n",
        "    return df.rename(index=str, columns={old_name: new_name})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y7E1ikLkGRri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inspect_df(df):\n",
        "    \"\"\"\n",
        "    shows key infos about the given dataframe \n",
        "    :param df: \n",
        "    :return: void \n",
        "    \"\"\"\n",
        "    print(\"Nr. of data: \" + str(len(df)))\n",
        "    print(\"Sample data: \")\n",
        "    print()\n",
        "    print(\"Meta Data\")\n",
        "    print(df.info())\n",
        "    print()\n",
        "    print(df.tail(3).T)\n",
        "    print()\n",
        "    show_correlation_matrix(df)\n",
        "    print(\"Correlation Matrix\")\n",
        "    print(df.corr())\n",
        "    print()\n",
        "    print(\"Nr. of data: \" + str(len(df)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nj4D8EbVG8Hq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data pre-processors "
      ]
    },
    {
      "metadata": {
        "id": "AQ5oudCtUMZL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_date(df, col_name):\n",
        "    \"\"\"\n",
        "    Converts the given date column from the usual object to an instance of datatime\n",
        "\n",
        "    :param df: pandas dataframe\n",
        "    :param col_name: date column\n",
        "    :return: dataframe with date column of type datetime\n",
        "    \"\"\"\n",
        "    # Passing infer_datetime_format=True can often-times speedup a parsing\n",
        "    # if its not an ISO8601 format exactly, but in a regular format.\n",
        "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\n",
        "    df = pd.to_datetime(df[col_name],  infer_datetime_format=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ceWeUAH7I_sL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def drop_features(df, features):\n",
        "    \"\"\"\n",
        "    Removes the collumn matching a features name\n",
        "    :param df: pandas data frame\n",
        "    :param  features: [String Array]\n",
        "    :return: Void - modifies the frame in place\n",
        "    \"\"\"\n",
        "    return df.drop(columns=features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AR5IX-kPG9ti",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_previous_values(df, column_name, number):\n",
        "    \"\"\" Adds n-previous values and stores each in a seperate column\n",
        "        According to findings by tsfresh, the previous value can have as much\n",
        "        as 88.5% significance on predicting the current value.\n",
        "        https://github.com/blue-yonder/tsfresh/blob/master/notebooks/timeseries_forecasting_google_stock.ipynb\n",
        "\n",
        "    :param df: data frame\n",
        "    :param column_name: source column\n",
        "    :param number: number of time periods to add\n",
        "    :return: None - modifies the frame in place\n",
        "    \"\"\"\n",
        "    for n in range(1, (number + 1)):\n",
        "        df[column_name + str(\"-\") + str(n)] = df[column_name].shift(-n)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8yYZg12YJYJU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_percent_change(df, column_name):\n",
        "    \"\"\"\n",
        "    Calculates the percentage change for each value in the given column\n",
        "    :param df: pandas data frame\n",
        "    :param column_name: String - name of the column \n",
        "    :return: Void - modifies the frame in place\n",
        "    \"\"\"\n",
        "    df[column_name + \"-pct-chng\"] = (df[column_name + \"-delta\"] / df[column_name]) * 100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8GhjK6ghJZxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_row_delta(df, column_name):\n",
        "    \"\"\"\n",
        "    calculates the difference between the current and the previous value in the given column \n",
        "    :param df: pandas data frame\n",
        "    :param column_name: String - name of the column\n",
        "    :return: Void - modifies the frame in place\n",
        "    \"\"\"\n",
        "    df[column_name + '-delta'] = df[column_name] - df[column_name].shift(-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-QwGTEglJuff",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_technical_indicators(df, column_name: str, id: int, all: bool, verbose: bool):\n",
        "    \"\"\"\n",
        "    Calculates a set of selecteed technical indicators based on the close price of the given stock\n",
        "    :param df: pandas data frame\n",
        "    :param column_name: MUST refer to the close price of the stock\n",
        "    :return: Void - modifies the frame in place\n",
        "    \"\"\"\n",
        "    close = np.asarray(df[column_name])\n",
        "    # This is for experimenting to generate a wide range of technical indicators\n",
        "    # requires subsequent feature ranking\n",
        "    full_range = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 27, 28, 29]\n",
        "    short_range = [3, 5, 7, 10, 15, 20, 25]\n",
        "    long_range = [50, 60, 70, 80, 100, 150, 200, 250]\n",
        "    range = full_range + long_range\n",
        "\n",
        "    if (id == 1 or all):\n",
        "        # Bollinger bands\n",
        "        df['UP_BB'], df['MID_BB'], df['LOW_BB'] = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
        "        if (verbose):\n",
        "            print(\"ID: \" + str(id))\n",
        "            print(\"Bollinger bands\")\n",
        "            print(df.corr())\n",
        "\n",
        "    if (id == 2 or all):\n",
        "        # Create Simple Moving Average\n",
        "        # Time range adjusted based on feature ranking for S&P500\n",
        "        periods = range  # [2, 3, 4, 5, 7]\n",
        "        for period in periods:\n",
        "            df['SMA-' + str(period)] = talib.SMA(close, timeperiod=period)\n",
        "        if (verbose):\n",
        "            print(\"ID: \" + str(id))\n",
        "            print(\"Simple Moving Average\")\n",
        "            print(df.corr())\n",
        "\n",
        "    if (id == 3 or all):\n",
        "        # Create Exponential moving average\n",
        "        # correlation drops at 30 and beyond\n",
        "        # time range adjusted based on feature ranking for S&P500\n",
        "        periods = range  # [6,7,9,12]\n",
        "        for period in periods:\n",
        "            df['EMA-' + str(period)] = talib.EMA(close, timeperiod=period)\n",
        "        if (verbose):\n",
        "            print(\"ID: \" + str(id))\n",
        "            print(\"Exponential Moving Average\")\n",
        "            print(df.corr())\n",
        "\n",
        "    if (id == 4 or all):\n",
        "\n",
        "        # Create Momentum\n",
        "        # no strong correlation for the MOM indicators was found, thus disabled.\n",
        "        # only MOM-300 yields about ~ -30% Corr.\n",
        "        periods = range\n",
        "        for period in periods:\n",
        "            df['MOM-' + str(period)] = talib.MOM(close, timeperiod=period)\n",
        "        if (verbose):\n",
        "            print(\"ID: \" + str(id))\n",
        "            print(\"Momentum\")\n",
        "            print(df.corr())\n",
        "\n",
        "    if (id == 5 or all):\n",
        "        # Create RSI\n",
        "        # Time range adjusted based on feature ranking for S&P500\n",
        "        periods = range  # [10, 11, 12, 13, 14, 15, 21, 22]\n",
        "        for period in periods:\n",
        "            df['RSI-' + str(period)] = talib.RSI(close, timeperiod=period)\n",
        "        if (verbose):\n",
        "            print(\"ID: \" + str(id))\n",
        "            print(\"RSI\")\n",
        "            print(df.corr())\n",
        "\n",
        "    if (id == 6 or all):\n",
        "        # Create TRIX\n",
        "        # Time range adjusted based on feature ranking for S&P500\n",
        "        # For a smaller sample size, only Trix-30 shows higehst correlation to close price.\n",
        "        # Add full range to re-test and look how Trix-30 performs\n",
        "        periods = range  # [3, 4]  # range\n",
        "        for period in periods:\n",
        "            df['TRIX-' + str(period)] = talib.TRIX(close, timeperiod=period)\n",
        "\n",
        "        if (verbose):\n",
        "            print(\"ID: \" + str(id))\n",
        "            print(\"Trix\")\n",
        "            print(df.corr())\n",
        "\n",
        "    if (id == 7 or all):\n",
        "        # Cycle Indicator Functions\n",
        "        # https://mrjbq7.github.io/ta-lib/func_groups/cycle_indicators.html\n",
        "        df[\"HT_DCPERIOD\"] = talib.HT_DCPERIOD(close)\n",
        "        df[\"HT_DCPHASE\"] = talib.HT_DCPHASE(close)\n",
        "        df[\"HT_TRENDMODE\"] = talib.HT_TRENDMODE(close)\n",
        "        if (verbose):\n",
        "            print(\"ID: \" + str(id))\n",
        "            print(\"Cycle Indicator Functions\")\n",
        "            print(df.corr())\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IpuZT_EALwmL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Processing pipline"
      ]
    },
    {
      "metadata": {
        "id": "ZL09ENqcL0ML",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_stock_data(verbose: bool, persist: bool):\n",
        "    \"\"\"\n",
        "\n",
        "    Sample pre-processing pipeline on the S&P500 50 Year data\n",
        "\n",
        "    The 50Y dataset contains open, high, low, close, volume [OHLCV] data and and percentage change.\n",
        "\n",
        "    Functions:\n",
        "    - calc_all_features requires [OHLCV] data\n",
        "\n",
        "    - get_chart_patterns adds all known chart patterns and requires [OHLCV] data\n",
        "\n",
        "    - calc_technical_indicators requires only the closing price\n",
        "\n",
        "    :param verbose: show details\n",
        "    :param persist: store processed data in train, test, and valid files\n",
        "    :return: void\n",
        "    \"\"\"\n",
        "\n",
        "    if (verbose):\n",
        "        print_versions()\n",
        "        print(\"load raw data ...\")\n",
        "    df = load_data(data=Data.SP500_50Y_RAW, force_download=True)\n",
        "\n",
        "    y_name = \"Close\"\n",
        "    if (verbose):\n",
        "        print(\"Set dependent variable to : \" + y_name)\n",
        "\n",
        "    if (verbose):\n",
        "        print(\"Convert date to datetime : \")\n",
        "    # That's required for automated feature engineering\n",
        "    #convert_date(df, \"Date\")\n",
        "    df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)\n",
        "   \n",
        "    if (verbose):\n",
        "        print(\"Categorify Date  : \")\n",
        "        add_datepart(df, \"Date\", drop=False)\n",
        "    \n",
        "    if (verbose):\n",
        "        print(\"Inspect Data: \")\n",
        "        print(df.info())\n",
        "        print()\n",
        "        print(df.tail(3).T)\n",
        "\n",
        "    if (verbose):\n",
        "        print(\"check raw data for missing values: \")\n",
        "\n",
        "    missing = check_missing_values(df=df, verbose=verbose)\n",
        "    # There should be done, but double check anyway\n",
        "    if (missing):\n",
        "        print(\"Found missing! Remove now... \")\n",
        "        df = remove_missing_values(df)\n",
        "        # Double check, should print zero\n",
        "        check_missing_values(df=df, verbose=verbose)\n",
        "\n",
        "    if (verbose):\n",
        "        print(\"add closing values of the previous n days\")\n",
        "    add_previous_values(df=df, column_name=y_name, number=5)\n",
        "\n",
        "    if (verbose):\n",
        "        print(\"Calculate technical indicators\")\n",
        "        # id = sets the technical indicator group\n",
        "        # all = do all technical indicators\n",
        "        # Verbose = prints out the correlation matrix for each group\n",
        "    calc_technical_indicators(df=df, column_name=y_name, id=1, all=True, verbose=False)\n",
        "\n",
        "\n",
        "    if (verbose):\n",
        "        print(\"Run feature engineering...\")\n",
        "\n",
        "        #@TODO: Have fun...\n",
        "\n",
        "        #add_previous_values(df=df, column_name=y_name, number=5)\n",
        "        #calc_technical_indicators(df=df, column_name=y_name, id=1, all=True, verbose=False)\n",
        "        # def get_chart_patterns(df)\n",
        "        print(\"Done feature engineering...\")\n",
        "\n",
        "\n",
        "    if (verbose):\n",
        "        print(\"Remove columns ... \")\n",
        "    # values of Adj close  are nearly identical to the close column\n",
        "    # Open, High, Low, aren't predicted.\n",
        "    rem_columns = [\"Adj Close\", \"Open\", \"High\", \"Low\"]\n",
        "    remove_column(df, rem_columns)\n",
        "\n",
        "    if (verbose):\n",
        "        inspect_df(df)\n",
        "\n",
        "    if (persist):\n",
        "        split = 0.80 # for a 80/20 split\n",
        "        valid_size = 90  # last 3 months for validation\n",
        "        save_train_test_valid(df=df, split_ratio=split, valid_size=valid_size, verbose=verbose)\n",
        "        print(\"Done: Data processing completed\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Di90K7aGMrSS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run processing pipeline"
      ]
    },
    {
      "metadata": {
        "id": "YW-jFzR_MtJL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "run = True\n",
        "download = False # set to true to download the generated datasets \n",
        "if run:\n",
        "  process_stock_data(verbose, persist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-zk9StX8Bqt_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Check new files \n"
      ]
    },
    {
      "metadata": {
        "id": "EnOpAxyBaYKV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load new dataset"
      ]
    },
    {
      "metadata": {
        "id": "KXfHK3XmTJGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if run: # only test the new files when the processor has generated them..\n",
        "  print(\"Load & insect train dataset\")\n",
        "  train_df = load_data(data=Data.SP500_TRAIN)\n",
        "  train_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rj6Qxx4Axhew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if run:\n",
        "  print(\"Load & insect test dataset\")\n",
        "  test_df = load_data(data=Data.SP500_TEST)\n",
        "  test_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KovJlD4qxqfh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if run:\n",
        "  print(\"Load & insect valid dataset\")\n",
        "  valid_df = load_data(data=Data.SP500_VALID)\n",
        "  valid_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PU0zbrRGXnZl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if run:\n",
        "  print(\"Show Train dataset\")\n",
        "  train_df.tail(3).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HVSCDII4z-b_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download new files "
      ]
    },
    {
      "metadata": {
        "id": "o-YXYyiK0Cif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if download:\n",
        "  # zip everything together \n",
        "  !zip SP500-data-clean.zip SP500-*\n",
        "  # Download\n",
        "  files.download('SP500-data-clean.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dA_Wc4fT1WTf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Upload file \n",
        "\n",
        "... just in case"
      ]
    },
    {
      "metadata": {
        "id": "-y3UNGnO0Yfm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://www.tecmint.com/unzip-extract-zip-files-to-specific-directory-in-linux/\n",
        "# Extract with !unzip SP500-data-clean.zip \n",
        "#from google.colab import files\n",
        "#files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}